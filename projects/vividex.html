<script src="http://www.google.com/jsapi" type="text/javascript"></script>

<style>
    .button {
        background-color: #4CAF50;
        /* Green */
        border: none;
        color: white;
        padding: 10px 20px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 5px 10px;
        cursor: pointer;
    }

    .button1 {
        border-radius: 5px;
        /* font-size: 18px; */
        background-color: white;
        color: black;
        border: 2px solid #2d6987
        /* Green */
    }

    .button2 {
        border-radius: 4px;
    }

    .button3 {
        border-radius: 8px;
    }

    .button4 {
        border-radius: 12px;
    }

    .button5 {
        border-radius: 50%;
    }


    .td-center {
        align: center;
        text-align: center;
        padding: 10px
    }

    .text_div {
        text-align: justify;
        text-justify: inter-word;
    }

    #blink {
        color: red;
        transition: 0.4s;
    }
    .crop {
        width: 650px;
        
        overflow: hidden;
    }
    .crop1 {
        width: 650px;
        margin: -75px 0 -75px 0px;
    }
</style>


<html lang="en">
    <head>
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-214815640-1"></script>
        <!-- <script type="text/javascript">google.load("jquery", "1.3.2");</script> -->
        <!-- <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-214815640-1');
        </script> -->
        <!-- <script src=”http://code.jquery.com/jquery-1.9.1.js”></script> -->
        <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel='icon' href='img/favicon.ico' type='image/x-icon'/>
        <meta name="description" content="ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos">
        <meta name="author" content="WILLOW team">
        <title>ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos</title>
        <link href="css/bootstrap.min.css" rel="stylesheet">
    </head>

    <body>
    <div class="container">

        <div style="height:20px;"></div>
        <div class="header">
            <h3>
                <center> <b>ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos</b> </center>
            </h3>
        </div>
        <div style="height:10px;"></div>

        <table align=center max-width=700px>
            <tr>
                <td align=center width=150px>
                    <center>
                        <span style="font-size:18px"><a href="https://zerchen.github.io/" target="_blank">Zerui Chen</a><sup>1</sup></span>
                    </center>
                </td>
                <td align=center width=200px>
                    <center>
                        <span style="font-size:18px"><a href="https://cshizhe.github.io/" target="_blank">Shizhe Chen</a><sup>1</sup></span>
                    </center>
                </td>
                <td align=center width=200px>
                    <center>
                        <span style="font-size:18px"><a href="https://www.di.ens.fr/willow/people_webpages/cordelia/" target="_blank">Cordelia Schmid</a><sup>1</sup></span>
                    </center>
                </td>
                <td align=center width=150px>
                    <center>
                        <span style="font-size:18px"><a href="https://www.di.ens.fr/~laptev/" target="_blank">Ivan Laptev</a><sup>2</sup></span>
                    </center>
                </td>
            </tr>
        </table>
        <div style="height:5px;"></div>

        <table align=center max-width=650px>
            <tr>
                <td align=center width=500px>
                    <center>
                        <span style="font-size:16px"><sup>1</sup>Inria, École normale supérieure, CNRS, PSL Research University</span>
                        <br>
                        <sup>2</sup> Mohamed bin Zayed University of Artificial Intelligence </span>
                    </center>
                </td>
            </tr>
        </table>
        <div style="height:10px;"></div>

        <table align=center width=700px>
			<tr>
				<td align=center width=700px>
					<center>
						<span style="font-size:16px">Under Review</span>
					</center>
				</td>
			</tr>
		</table>
		<div style="height:10px;"></div>

        <div class="links" style="font-weight:bold; text-align:center">
            <a class="btn btn-info" href="https://arxiv.org/abs/2404.15709" target="_blank">Paper</a>
            &nbsp;&nbsp;&nbsp;
            <a class="btn btn-info" href="#bib">BibTex</a>
            &nbsp;&nbsp;&nbsp;
            <a class="btn btn-info" href="" target="_blank">Code</a>
            &nbsp;&nbsp;&nbsp;
        </div>

        <hr>

        <div class="row" id="abstract" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Abstract</h3>
            <p style="text-align: justify;">
                In this work, we aim to learn a unified vision-based policy for a multi-fingered robot hand to manipulate different objects in diverse poses. Though prior work has demonstrated that human videos can benefit policy learning, performance improvement has been limited by physically implausible trajectories extracted from videos. Moreover, reliance on privileged object information such as ground-truth object states further limits the applicability in realistic scenarios. To address these limitations, we propose a new framework ViViDex to improve vision-based policy learning from human videos. It first uses reinforcement learning with trajectory guided rewards to train state-based policies for each video, obtaining both visually natural and physically plausible trajectories from the video. We then rollout successful episodes from state-based policies and train a unified visual policy without using any privileged information. A coordinate transformation method is proposed to significantly boost the performance. We evaluate our method on three dexterous manipulation tasks and demonstrate a large improvement over state-of-the-art algorithms.
	        </p>
        </div><!-- Import the component -->

        <hr>

        <div class="row" id="method" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Method</h3><br>
            <center>
                <img src="./resources/vividex_assets/vividex_overview.png" width="100%">
            </center>
            <br>
            <p style="text-align: justify;">
                <b>Our method.</b> The overall framework of our method for learning dexterous manipulation skills from human videos. It consists of three modules: the extraction of the reference trajectory from human videos, the trajectory-guided state-based policy learning using RL, and the vision-based policy learning using behavior cloning.
            </p>
            <br>
        </div>

        <hr>

        <div class="row" id="result_video" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Introduction Video and Qualitative Results</h3><br>
            <center>
            <video width="80%" controls>
                <source src="./resources/vividex_assets/vividex.webm" type="video/mp4" allow="autoplay" />
            </video>
            </center>
        </div>

        <hr>

	 <div class="row" id="bib" style="max-width:1000px; margin:0 auto; text-align:justify">
	    <h3>BibTeX</h3>
	       <pre><tt>@InProceedings{chen2024vividex,
author       = {Chen, Zerui and Chen, Shizhe and Schmid, Cordelia and Laptev, Ivan},
title        = {{ViViDex}: Learning Vision-based Dexterous Manipulation from Human Videos},
booktitle    = {arXiv:2404.15709},
year         = {2024},
}</tt></pre>
	</div> 

        <div class="row" id="acknowledgements" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Acknowledgements</h3>
            <p>
	       This work was granted access to the HPC resources of IDRIS under the allocation AD011013147 made by GENCI. It was funded in part by the French government under management of Agence Nationale de la Recherche as part of the “Investissements d’avenir” program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute) and the ANR project VideoPredict (ANR-21-FAI1-0002-01).
            </p>
        </div>

        <hr>

        <div class="row" id="copyright" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Copyright</h3>
            <p>
                The documents contained in these directories are included by the contributing authors as a means to ensure timely dissemination of scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright.
            </p>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-3 col-sm-9">
                <center>
                   <img src="resources/inria_logo.png" width="150">
                </center>
            </div>
            <div class="col-md-3 col-sm-9">
                <center>
                   <img src="resources/ens_logo.png" width="150">
                </center>
            </div>
        </div>

        <br><br>
        <script src="./resources/alignsdf_assets/main.js"></script>
        <script src="./resources/alignsdf_assets/interaction.js"></script>
        <script src="./resources/alignsdf_assets/model_click.js"></script>
        <script>
            showDemo(1)
            modelEventListeners()
            enableInteraction()
        </script>
   </body>
</html>
