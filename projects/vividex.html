<script src="http://www.google.com/jsapi" type="text/javascript"></script>

<style>
    .button {
        background-color: #4CAF50;
        /* Green */
        border: none;
        color: white;
        padding: 10px 20px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;
        margin: 5px 10px;
        cursor: pointer;
    }

    .button1 {
        border-radius: 5px;
        /* font-size: 18px; */
        background-color: white;
        color: black;
        border: 2px solid #2d6987
        /* Green */
    }

    .button2 {
        border-radius: 4px;
    }

    .button3 {
        border-radius: 8px;
    }

    .button4 {
        border-radius: 12px;
    }

    .button5 {
        border-radius: 50%;
    }


    .td-center {
        align: center;
        text-align: center;
        padding: 10px
    }

    .text_div {
        text-align: justify;
        text-justify: inter-word;
    }

    #blink {
        color: red;
        transition: 0.4s;
    }
    .crop {
        width: 650px;
        
        overflow: hidden;
    }
    .crop1 {
        width: 650px;
        margin: -75px 0 -75px 0px;
    }
</style>


<html lang="en">
    <head>
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-214815640-1"></script>
        <!-- <script type="text/javascript">google.load("jquery", "1.3.2");</script> -->
        <!-- <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-214815640-1');
        </script> -->
        <!-- <script src=”http://code.jquery.com/jquery-1.9.1.js”></script> -->
        <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel='icon' href='img/favicon.ico' type='image/x-icon'/>
        <meta name="description" content="ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos">
        <meta name="author" content="WILLOW team">
        <title>ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos</title>
        <link href="css/bootstrap.min.css" rel="stylesheet">
    </head>

    <body>
    <div class="container">

        <div style="height:20px;"></div>
        <div class="header">
            <h3>
                <center> <b>ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos</b> </center>
            </h3>
        </div>
        <div style="height:10px;"></div>

        <table align=center max-width=700px>
            <tr>
                <td align=center width=150px>
                    <center>
                        <span style="font-size:18px"><a href="https://zerchen.github.io/" target="_blank">Zerui Chen</a><sup>1</sup></span>
                    </center>
                </td>
                <td align=center width=200px>
                    <center>
                        <span style="font-size:18px"><a href="https://cshizhe.github.io/" target="_blank">Shizhe Chen</a><sup>1</sup></span>
                    </center>
                </td>
                <td align=center width=200px>
                    <center>
                        <span style="font-size:18px"><a href="https://scholar.google.com/citations?user=-0kdc5cAAAAJ&hl=fr" target="_blank">Etienne Arlaud</a><sup>1</sup></span>
                    </center>
                </td>
                <td align=center width=150px>
                    <center>
                        <span style="font-size:18px"><a href="https://www.di.ens.fr/~laptev/" target="_blank">Ivan Laptev</a><sup>2</sup></span>
                    </center>
                </td>
                <td align=center width=200px>
                    <center>
                        <span style="font-size:18px"><a href="https://cordeliaschmid.github.io/" target="_blank">Cordelia Schmid</a><sup>1</sup></span>
                    </center>
                </td>
            </tr>
        </table>
        <div style="height:5px;"></div>

        <table align=center max-width=650px>
            <tr>
                <td align=center width=500px>
                    <center>
                        <span style="font-size:16px"><sup>1</sup>Inria, École normale supérieure, CNRS, PSL Research University</span>
                        <br>
                        <sup>2</sup> Mohamed bin Zayed University of Artificial Intelligence </span>
                    </center>
                </td>
            </tr>
        </table>
        <div style="height:10px;"></div>

        <table align=center width=700px>
			<tr>
				<td align=center width=700px>
					<center>
						<span style="font-size:16px">Accepted at <a href="https://2025.ieee-icra.org/" target="_blank">ICRA, 2025</a> </span>
					</center>
				</td>
			</tr>
		</table>
		<div style="height:10px;"></div>

        <div class="links" style="font-weight:bold; text-align:center">
            <a class="btn btn-info" href="https://arxiv.org/abs/2404.15709" target="_blank">Paper</a>
            &nbsp;&nbsp;&nbsp;
            <a class="btn btn-info" href="#bib">BibTex</a>
            &nbsp;&nbsp;&nbsp;
            <!-- <a class="btn btn-info" href="" target="_blank">Code</a>
            &nbsp;&nbsp;&nbsp; -->
        </div>

        <hr>

        <div class="row" id="abstract" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Abstract</h3>
            <p style="text-align: justify;">
                In this work, we aim to learn a unified vision-based policy for multi-fingered robot hands to manipulate a variety of objects in diverse poses. Though prior work has shown benefits of using human videos for policy learning, performance gains have been limited by the noise in estimated trajectories. Moreover, reliance on privileged object information such as ground-truth object states further limits the applicability in realistic scenarios. To address these limitations, we propose a new framework ViViDex to improve vision-based policy learning from human videos. It first uses reinforcement learning with trajectory guided rewards to train state-based policies for each video, obtaining both visually natural and physically plausible trajectories from the video. We then rollout successful episodes from state-based policies and train a unified visual policy without using any privileged information. We propose coordinate transformation to further enhance the visual point cloud representation, and compare behavior cloning and diffusion policy for the visual policy training. Experiments both in simulation and on the real robot demonstrate that ViViDex outperforms state-of-the-art approaches on three dexterous manipulation tasks.
	        </p>
        </div><!-- Import the component -->

        <hr>

        <div class="row" id="method" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Overview of our method</h3><br>
            <center>
                <img src="./resources/vividex_assets/vividex_overview.png" width="100%">
            </center>
            <br>
            <p style="text-align: justify;">
                <b>Our method.</b> The overall framework of our method for learning dexterous manipulation skills from human videos. It consists of three steps: extraction of reference trajectories from human videos, trajectory-guided state-based policy learning using RL, and vision-based policy learning using either the behavior cloning or the 3D diffusion policy.
            </p>
            <br>
        </div>

        <hr>

        <div class="row" id="introduction_video" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Introduction video</h3><br>
            <center>
            <video width="100%" controls>
                <source src="./resources/vividex_assets/vividex.webm" type="video/mp4" allow="autoplay" />
            </video>
            </center>
        </div>

        <hr>

        <div class="row" id="result_video_real" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Qualitative results on the real robot</h3><br>
            <center>
            <video width="100%" controls>
                <source src="./resources/vividex_assets/vividex_real.webm" type="video/mp4" allow="autoplay" />
            </video>
            </center>
        </div>

        <hr>

        <div class="row" id="result_video_simu" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Qualitative results in simulation</h3><br>
            <center>
            <video width="100%" controls>
                <source src="./resources/vividex_assets/vividex_simu.webm" type="video/mp4" allow="autoplay" />
            </video>
            </center>
        </div>

        <hr>

	 <div class="row" id="bib" style="max-width:1000px; margin:0 auto; text-align:justify">
	    <h3>BibTeX</h3>
	       <pre><tt>@article{chen2024vividex,
author       = {Chen, Zerui and Chen, Shizhe and Arlaud Etienne and Laptev, Ivan and Schmid, Cordelia},
title        = {{ViViDex}: Learning Vision-based Dexterous Manipulation from Human Videos},
booktitle    = {arXiv:2404.15709},
year         = {2024},
}</tt></pre>
	</div> 

        <div class="row" id="acknowledgements" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Acknowledgements</h3>
            <p>
	       We thank Ricardo Garcia Pinel for help with the camera calibration, Yuzhe Qin for clarifications about DexMV and suggestions on the real robot depolyment. This work was granted access to the HPC resources of IDRIS under the allocation AD011013147 made by GENCI. It was funded in part by the French government under management of Agence Nationale de la Recherche as part of the “Investissements d’avenir” program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute) and the ANR project VideoPredict (ANR-21-FAI1-0002-01).
            </p>
        </div>

        <hr>

        <div class="row" id="copyright" style="max-width:1000px; margin:0 auto; text-align:justify">
            <h3>Copyright</h3>
            <p>
                The documents contained in these directories are included by the contributing authors as a means to ensure timely dissemination of scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright.
            </p>
        </div>

        <hr>

        <div class="row">
            <div class="col-md-3 col-sm-9">
                <center>
                   <img src="resources/inria_logo.png" width="150">
                </center>
            </div>
            <div class="col-md-3 col-sm-9">
                <center>
                   <img src="resources/ens_logo.png" width="150">
                </center>
            </div>
        </div>

        <br><br>
        <script src="./resources/alignsdf_assets/main.js"></script>
        <script src="./resources/alignsdf_assets/interaction.js"></script>
        <script src="./resources/alignsdf_assets/model_click.js"></script>
        <script>
            showDemo(1)
            modelEventListeners()
            enableInteraction()
        </script>
   </body>
</html>
